{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "### *Practicum AI:* Transfer - Fruit Classification with Transfer Learning\n",
    "\n",
    "This exercise was adapted from Baig et al. (2020) <i> The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Activity 3.02, page 150).\n",
    "\n",
    "(10 Minutes)\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "In this exercise, we will train a CNN to recognize images of fruits from 120 different classes. To accomplish this, we will employ transfer learning and data augmentation. We use the [Fruits 360 dataset](https://arxiv.org/abs/1712.00580), which was shared by Horea Muresea and Mihai Oltean, *Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol.10, Issue 1, pp.26-42, 2018*.\n",
    "\n",
    "There are 90483 photos of 131 fruits and vegetables in the original collection. We will use a subset of this dataset, which contains over 16,000 photos from 120 different classes.\n",
    "\n",
    "#### 1. Import requisite libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import the dataset and unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Activity3.02/fruits360.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `tf.keras.get_file` with `'fruits360.zip'`, download the dataset and save the results to a variable called **zip_dir**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/PacktWorkshops/The-Deep-Learning-Workshop/raw/master/Chapter03/Datasets/Activity3.02/fruits360.zip\n",
      "82223104/82220233 [==============================] - 2s 0us/step\n",
      "82231296/82220233 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "zip_dir = tf.keras.utils.get_file('fruits360.zip', \n",
    "                                   origin       = file_url, \n",
    "                                   extract      = True, \n",
    "                                   cache_dir    = '/blue/rc-workshops/danielmaxwell',\n",
    "                                   cache_subdir = 'keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> If you've already downloaded the dataset, subsequent downloads will be slow as the get_file() function updates existing images one at a time.  The function works but is incredibly inefficient.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prepare the data for training\n",
    "\n",
    "Using pathlib, create a variable named **path** that contains the full path to the fruits360_filtered directory by `pathlib.Path().parent` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/blue/rc-workshops/danielmaxwell/keras/fruits360_filtered\n"
     ]
    }
   ],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the variables **train_dir** and **validation_dir**, which store the full paths of the train (for training) and validation (for test) folders, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = path / 'Training'\n",
    "validation_dir = path / 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of images in the train and validation datasets are set at 11398 and 4752, respectively.\n",
    "\n",
    "```python\n",
    "total_train = 11398\n",
    "total_val = 4752\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation ImageDataGenerator using `ImageDataGenerator` with data augmentation. The train data will be rescaled, rotated, shifted, and flipped, however the validation data will only be rescaled.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_image_generator = ImageDataGenerator(rescale = 1./255, rotation_range = 40, width_shift_range = 0.1, height_shift_range = 0.1, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, fill_mode = 'nearest')\n",
    "\n",
    "validation_image_generator = ImageDataGenerator(rescale = 1./255)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create four variables **batch_size**, **img_height**, **img_width**, and **channel**.\n",
    "\n",
    "```python\n",
    "batch_size = 16\n",
    "img_height = 100\n",
    "img_width  = 100\n",
    "channel    = 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create train and validation data generators, use `.flow_from_directory()` and give the batch size, directory, and target size.\n",
    "\n",
    "```python\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size  = batch_size, directory = train_dir, target_size = (img_height, img_width))\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size  = batch_size, directory = validation_dir, target_size = (img_height, img_width))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Load the pre-trained VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set 8 as the seed for numpy and tensorflow using `np.random_seed()` and `tf.random.set_seed()`.\n",
    "\n",
    "```python\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained model VGG16 from `tensorflow.kreas.applications`.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import VGG16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a variable called **base_model** and populate it with the following parameters for the VGG16 model.\n",
    "\n",
    "```python\n",
    "base_model = VGG16(input_shape = (img_height, img_width, channel), weights = 'imagenet', include_top = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Freeze all the layers in the base VGG16 model\n",
    "\n",
    "Set the model to non-trainable using the `.trainable` attribute.\n",
    "\n",
    "```python\n",
    "base_model.trainable =  False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the summary of the VGG16 model.\n",
    "\n",
    "```python\n",
    "base_model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows us the architecture of VGG16. We can see that there are 14,714,688 parameters in total, but the trainable parameters are zero. This is because we have frozen all the layers of the VGG16 model.\n",
    "\n",
    "#### 6. Add custom classification layers\n",
    "\n",
    "Create our new model using `tf.kera.Sequential()` by adding the following layers to the base model:\n",
    "* A Flatten layer.\n",
    "* A Dense layer with 1000 units and ReLU activation.\n",
    "* The final Dense layer with 120 neurons and softmax activation.\n",
    "\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1000, activation = 'relu'),\n",
    "    layers.Dense(120, activation = 'softmax')\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Compile the model for training\n",
    "\n",
    "The model should be compiled with the Adam optimizer with a learning rate of 0.001, categorical crossentropy loss, and accuracy metric.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Train the Model\n",
    "Train the model with `.fit()` and provide the train and validation data, the steps per epoch, total train epochs, and the validation steps.\n",
    "\n",
    "```python\n",
    "model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch  = total_train // batch_size,\n",
    "    epochs = 5,\n",
    "    validation_data  = val_data_gen,\n",
    "    validation_steps = total_val // batch_size\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used transfer learning to customize a pre-trained VGG16 model from ImageNet to our fruit classification dataset. We replaced the head of the model with our own fully connected layers and trained them over five epochs. The accuracy scores for both training and validation are quite remarkable. You also can try fine-tuning this model by changing some of the parameters and see if you can get a higher score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
