{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d88515b",
   "metadata": {},
   "source": [
    "# Transfer Learning Concepts\n",
    "\n",
    "You may recall *Practicum AI*\"s heroine Amelia, the AI-savvy nutritionist. At the end of our *[Deep Learning Foundations course](https://practicumai.org/courses/deep_learning/)*, Amelia was helping with a computer vision project. Her colleague, an entomologist named Kevin, had a dataset of images of bees and wasps and wanted to classify them.\n",
    "\n",
    "![Image of bees and wasps from the dataset cover image](https://github.com/PracticumAI/deep_learning/blob/main/images/bees_wasps_dataset-cover.png?raw=true)\n",
    "\n",
    "\n",
    "## AI Pathway review for Bees vs Wasps\n",
    "\n",
    "If you have taken our [Getting Started with AI course](https://practicumai.org/courses/getting_started/), you may remember this figure of the AI Application Development Pathway. Let\"s take a quick review of how we applied this pathway in the case of the Bees vs Wasps example.\n",
    "\n",
    "![AI Application Development Pathway image showing the 7 steps in developing an AI application](https://practicumai.org/getting_started/images/application_dev_pathway.png)\n",
    "\n",
    "1. **Choose a problem to solve:** In this example, we need to classify images as bees, wasps, other insects, or a non-insect. \n",
    "2. **Gather data:** The data for the example come from [Kaggle](https://www.kaggle.com/), a great repository of datasets, code, and models.\n",
    "3. **Clean and prepare the data:** In the *Deep Learning Foundations* course, we assumed that this was done for us. One issue that we ran into was that of class imbalance. There are many more images in some classes than others, leading to a poor performing model.\n",
    "4. **Choose a model:** In the *Deep Learning Foundations* course, we presented the model with little detail. Now that we know more about Convolutional Neural Networks and some other tools at our disposal, we will explore the model in more detail.\n",
    "   * As part of the iterative process among this and the next steps, one thing we noticed is that most of our models were overfitting â€” performing better on the training data than they did on the testing data. Essentially, the models memorized the training data but did not generalize well to new data that had not been seen. \n",
    "      * In this notebook, we will explore **dropout** as one mechanism to mitigate overfitting.\n",
    "5. **Train the model:** In training the model we may have had a few issues. With so many hyperparameters to tune, it\"s easy to lose track of what combinations have been tried and how changes impacted model performance. \n",
    "   * In this notebook, we introduce you to [TensorBoard](https://www.tensorflow.org/tensorboard), one popular tool in a class of tools known as **experiment tracking** or **MLOps (Machine learning operations) tools**. These tools help track changes to hyperparameters, the training process, and the data. They allow comparison among runs and can even automate multiple runs for you. Learning to use MLOps tools will help you as you continue to learn more about AI workflows.\n",
    "6. **Evaluate the model:** We will continue to assess how the model performs on the test set and adjust the model and hyperparameters to attempt to produce a better model. However, as noted above in step 3, one issue is the class imbalance.    \n",
    "   * This is a common issue with real data, and in notebook [02.1_data_imbalance.ipynb](02.1_data_imbalance.ipynb), we will explore some methods to handle this.\n",
    "7. **Deploy the model:** We won\"t get to this stage in this exercise, but hopefully, we will end up with a model that could be deployed and achieve relatively good accuracy at solving the problem.\n",
    "\n",
    "\n",
    "## A refresher\n",
    "\n",
    "If you need a refresher, or haven\"t taken the *Deep Learning Foundations* course, the final notebook is part of this repository: [DLF_03_bees_vs_wasps.ipynb](DLF_03_bees_vs_wasps.ipynb). No need to worry though, we will cover what we did before and the new changes as we work through this notebook. Some of the code has been moved into the [helpers_01.py](helpers_01.py) file and is imported below to keep things cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What, Why, and How: Transfer Learning in Agriculture\n",
    "\n",
    "**What:**\n",
    "Transfer learning involves using a model pre-trained on one task and adapting it to a new, related task. In this notebook, we apply transfer learning to agricultural applications, such as plant disease detection.\n",
    "\n",
    "**Why:**\n",
    "Training a model from scratch requires extensive data and computational resources, which are often limited in specialized fields like agriculture. Transfer learning allows us to leverage pre-trained models to achieve high performance with fewer resources.\n",
    "\n",
    "**How:**\n",
    "We'll demonstrate three approaches:\n",
    "- Training a baseline model from scratch.\n",
    "- Fine-tuning a model pre-trained on ImageNet.\n",
    "- Fine-tuning a model pre-trained on AgriNet, a domain-specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What, Why, and How: Data Preparation\n",
    "\n",
    "**What:**\n",
    "We'll prepare a subset of the AgriNet dataset, focusing on plant disease detection. This includes loading images, preprocessing them, and splitting the data into training, validation, and test sets.\n",
    "\n",
    "**Why:**\n",
    "High-quality data preparation ensures that our models are trained on consistent, representative datasets, leading to better generalization and performance.\n",
    "\n",
    "**How:**\n",
    "Using TensorFlow's `ImageDataGenerator`, we'll preprocess images by resizing them to 224x224 pixels, normalizing pixel values, and applying a train-validation split. Augmentation techniques like rotation and flipping will also be applied to increase data variety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning in Agricultural Applications: A Case Study\n",
    "\n",
    "This notebook demonstrates the power of transfer learning in agricultural applications using the AgriNet dataset and pre-trained models. We'll compare the performance of:\n",
    "- A model trained from scratch on a subset of the data.\n",
    "- A model fine-tuned using ImageNet pre-trained weights.\n",
    "- A model fine-tuned using AgriNet pre-trained weights.\n",
    "\n",
    "By the end, you'll gain insights into how domain-specific pre-trained models can enhance performance in agricultural tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What, Why, and How: Baseline Model\n",
    "\n",
    "**What:**\n",
    "We'll train a simple convolutional neural network (CNN) from scratch as a baseline for comparison.\n",
    "\n",
    "**Why:**\n",
    "The baseline provides a performance benchmark, allowing us to assess the impact of transfer learning.\n",
    "\n",
    "**How:**\n",
    "We'll define a CNN with basic layers, such as convolutional, pooling, and fully connected layers. The model will be compiled with the Adam optimizer and categorical cross-entropy loss, then trained on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will load a subset of the AgriNet dataset, specifically focused on plant disease detection. The dataset will be split into training, validation, and test sets for model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What, Why, and How: Transfer Learning with ImageNet\n",
    "\n",
    "**What:**\n",
    "We'll use the VGG19 model pre-trained on ImageNet and fine-tune it for plant disease detection.\n",
    "\n",
    "**Why:**\n",
    "ImageNet pre-trained models have learned general features (e.g., edges, textures) that can be adapted to our specific task. This significantly reduces the training time and data requirements.\n",
    "\n",
    "**How:**\n",
    "The base layers of VGG19 will be frozen to retain their pre-trained features. We'll add custom layers for classification and fine-tune the model on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths to dataset\n",
    "data_dir = \"/path/to/agri_subset\"  # Replace with actual path\n",
    "\n",
    "# Define data generators\n",
    "datagen = ImageDataGenerator(rescale=1.0 / 255, validation_split=0.2)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    data_dir, target_size=(224, 224), batch_size=32, subset=\"training\"\n",
    ")\n",
    "\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    data_dir, target_size=(224, 224), batch_size=32, subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What, Why, and How: Transfer Learning with AgriNet\n",
    "\n",
    "**What:**\n",
    "We'll use the VGG19 model pre-trained on the AgriNet dataset, which is domain-specific to agriculture.\n",
    "\n",
    "**Why:**\n",
    "Domain-specific pre-training captures features relevant to agricultural tasks, such as plant patterns and disease characteristics, which can further improve model performance compared to generic pre-trained models.\n",
    "\n",
    "**How:**\n",
    "Similar to the ImageNet approach, we'll freeze the base layers of the AgriNet model, add custom classification layers, and fine-tune the model on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "We'll train a simple convolutional neural network from scratch and use it as our baseline for performance comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What, Why, and How: Performance Comparison\n",
    "\n",
    "**What:**\n",
    "We'll compare the performance of the three models (baseline, ImageNet pre-trained, and AgriNet pre-trained) using metrics like accuracy and F1-score.\n",
    "\n",
    "**Why:**\n",
    "This step helps quantify the benefits of transfer learning and highlights the impact of using domain-specific pre-trained models.\n",
    "\n",
    "**How:**\n",
    "We'll evaluate each model on the test set and visualize the results using performance metrics and charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define baseline model\n",
    "baseline_model = Sequential(\n",
    "    [\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=(224, 224, 3)),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(len(train_gen.class_indices), activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the baseline model\n",
    "history_baseline = baseline_model.fit(train_gen, validation_data=val_gen, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Key Insights\n",
    "\n",
    "- Transfer learning significantly improves performance compared to training from scratch, especially with limited data.\n",
    "- Domain-specific pre-training (e.g., AgriNet) can further enhance accuracy and generalization for specialized tasks.\n",
    "- These findings demonstrate the importance of transfer learning in tackling real-world challenges in agriculture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with ImageNet\n",
    "\n",
    "We'll use a pre-trained VGG19 model with ImageNet weights and fine-tune it on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "# Load pre-trained VGG19 model\n",
    "imagenet_model = VGG19(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze base layers\n",
    "for layer in imagenet_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers\n",
    "x = GlobalAveragePooling2D()(imagenet_model.output)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(len(train_gen.class_indices), activation=\"softmax\")(x)\n",
    "\n",
    "imagenet_model = Model(inputs=imagenet_model.input, outputs=output)\n",
    "\n",
    "imagenet_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_imagenet = imagenet_model.fit(train_gen, validation_data=val_gen, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with AgriNet\n",
    "\n",
    "We'll use the AgriNet pre-trained VGG19 model to fine-tune it on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming AgriNet weights are available locally\n",
    "agri_weights_path = \"/path/to/agri_vgg19_weights.h5\"  # Replace with actual path\n",
    "\n",
    "# Load the VGG19 model\n",
    "agri_model = VGG19(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Load AgriNet weights\n",
    "agri_model.load_weights(agri_weights_path)\n",
    "\n",
    "# Freeze base layers\n",
    "for layer in agri_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom top layers\n",
    "x = GlobalAveragePooling2D()(agri_model.output)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(len(train_gen.class_indices), activation=\"softmax\")(x)\n",
    "\n",
    "agri_model = Model(inputs=agri_model.input, outputs=output)\n",
    "\n",
    "agri_model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history_agri = agri_model.fit(train_gen, validation_data=val_gen, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare the performance metrics (accuracy, F1-score) of the three models:\n",
    "- Baseline model (trained from scratch)\n",
    "- Transfer learning with ImageNet pre-trained weights\n",
    "- Transfer learning with AgriNet pre-trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the benefits of transfer learning in agricultural tasks. The AgriNet pre-trained model outperformed the ImageNet model and the baseline, showing the importance of domain-specific pre-training for specialized applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
